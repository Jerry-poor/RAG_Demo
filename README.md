# RAG_Demo ğŸ§ ğŸ”  
ä¸€ä¸ªå°å°çš„å¨±ä¹é¡¹ç›® ğŸ®âœ¨  

This is a lightweight demo project for exploring Retrieval-Augmented Generation (RAG).  
è¿™æ˜¯æˆ‘ç”¨æ¥æ¢ç´¢æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„è½»é‡åŒ–æ¼”ç¤ºé¡¹ç›®ã€‚  

I use FAISS to build a simple vector database. ğŸ“¦  
æˆ‘ä½¿ç”¨ FAISS æ­å»ºäº†ä¸€ä¸ªç®€æ˜“çš„å‘é‡æ•°æ®åº“ã€‚  

The embedding model is `ibm-granite/granite-embedding-107m-multilingual`, which supports multiple languages. ğŸŒ  
ä½¿ç”¨çš„ embedding æ¨¡å‹æ˜¯ `ibm-granite/granite-embedding-107m-multilingual`ï¼Œæ”¯æŒå¤šç§è¯­è¨€ã€‚  

The LLM backend is powered by DeepSeek-R1. ğŸš€  
å¤§è¯­è¨€æ¨¡å‹éƒ¨åˆ†ä½¿ç”¨çš„æ˜¯ DeepSeek-R1ã€‚  

Note: Neither DeepSeek-R1 nor the IBM embedding model supports multi-modal input. ğŸ™…â€â™‚ï¸ğŸ–¼ï¸  
æ³¨æ„ï¼šDeepSeek-R1 å’Œ IBM çš„ embedding æ¨¡å‹éƒ½ä¸æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ã€‚  

So the multi-modal feature is currently just a placeholder for future extension. ğŸ§©  
å› æ­¤ç›®å‰çš„å¤šæ¨¡æ€éƒ¨åˆ†åªæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œæœªæ¥æœ‰éœ€è¦æ—¶ä¼šç»§ç»­æ‰©å±•ã€‚  

I integrated a lightweight OCR tool to simulate basic multi-modal functionality. ğŸ“„â¡ï¸ğŸ”¤  
æˆ‘ä½¿ç”¨äº†ä¸€ä¸ªè½»é‡çº§çš„ OCR åº“æ¥åšä¸€ä¸ªç®€å•çš„å¤šæ¨¡æ€æ¼”ç¤ºã€‚  

Running large models locally can be computationally expensive. ğŸ§®ğŸ’»  
æœ¬åœ°è¿è¡Œå¤§æ¨¡å‹çš„ç®—åŠ›å¼€é”€è¾ƒå¤§ã€‚  

For now, I use DeepSeekâ€™s API to build this demo and practice workflow. ğŸ”§ğŸ“¡  
ç›®å‰æˆ‘é€šè¿‡è°ƒç”¨ DeepSeek API æ¥åšè¿™ä¸ªæ¼”ç¤ºé¡¹ç›®ï¼Œä¹Ÿé¡ºä¾¿ç»ƒä¹ ä¸€ä¸‹æµç¨‹ã€‚  

I might consider full local deployment in the future if needed. ğŸ ğŸ¤”  
å°†æ¥æœ‰éœ€è¦çš„è¯ï¼Œæˆ‘ä¼šè€ƒè™‘åšå…¨æœ¬åœ°éƒ¨ç½²ã€‚  
